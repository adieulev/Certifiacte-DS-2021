{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/adieulev/Certificate-DS-2021/blob/main/Labs/Lab2021_RF_withQ.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uv8-L6LzKfpl",
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#-Random-forests-&amp;-Neural-networks...\" data-toc-modified-id=\"-Random-forests-&amp;-Neural-networks...-0.1\"><span class=\"toc-item-num\">0.1&nbsp;&nbsp;</span><font color=\"darkorange\"> Random forests &amp; Neural networks...</font></a></span></li></ul></li><li><span><a href=\"#1.---Random-forests-\" data-toc-modified-id=\"1.---Random-forests--1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>1. <font color=\"darkred\">  Random forests </font></a></span><ul class=\"toc-item\"><li><span><a href=\"#--Random-forests-for-time-series-prediction---simulated-data-\" data-toc-modified-id=\"--Random-forests-for-time-series-prediction---simulated-data--1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span><font color=\"darkred\">  Random forests for time series prediction - simulated data </font></a></span></li><li><span><a href=\"#-For-a-few-cells,-you-have-to-complete-a-few-lines-of-code.-The-solution-is-always-given-in-the-cell-just-below.\" data-toc-modified-id=\"-For-a-few-cells,-you-have-to-complete-a-few-lines-of-code.-The-solution-is-always-given-in-the-cell-just-below.-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span><font color=\"darkorange\"> For a few cells, you have to complete a few lines of code. The solution is always given in the cell just below.</font></a></span></li><li><span><a href=\"#A-first-Tree-regressor\" data-toc-modified-id=\"A-first-Tree-regressor-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>A first Tree regressor</a></span></li><li><span><a href=\"#Regression-Forest\" data-toc-modified-id=\"Regression-Forest-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Regression Forest</a></span></li></ul></li><li><span><a href=\"#-Random-forests-for-time-series-prediction---Inflation-prediction-\" data-toc-modified-id=\"-Random-forests-for-time-series-prediction---Inflation-prediction--2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span><font color=\"darkred\"> Random forests for time series prediction - Inflation prediction </font></a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/adieulev/Certificate-DS-2021/blob/main/Labs/Lab2021_RF_withQ.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "km0J3yRZKwB_"
   },
   "source": [
    "# <font color=darkcyan>  Machine Learning in Python </font>\n",
    "\n",
    "### <font color=darkorange> Random forests & Neural networks...</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Qmebz58KwCF"
   },
   "source": [
    "## 1. <font color=darkred>  Random forests </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AHSBnbN0KwCG"
   },
   "source": [
    "### <font color=darkred>  Random forests for time series prediction - simulated data </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zrdt6eshKwCG"
   },
   "source": [
    "Consider a Markov chain, given by $X_0 = x_0$ for $x_0\\in \\mathbb{R}^d$ and, for $k\\geqslant 0$,\n",
    "$$\n",
    "X_{k+1} = \\rho X_k + \\sigma_X\\varepsilon_k\\,\\,\\, [2\\pi]\\,\n",
    "$$ \n",
    "where $(\\epsilon_k)_{k\\geqslant 0}$ are i.i.d. standard Gaussian vectors in $\\mathbb{R}^d$ $\\sim {\\sf N}(0,{\\bf I}_d)$.\n",
    "\n",
    "The observation model is\n",
    "$$ \n",
    "Y_k = f(X_k) + \\sigma_Y\\eta_{k}\\,,\n",
    "$$\n",
    "where $(\\eta_k)_{k\\geqslant 0}$ are i.i.d. $\\sim N(0,1)$ and\n",
    "$$\n",
    "f:\\begin{cases}\n",
    "\\mathbb{R}^d\\to\\mathbb{R}\\\\\n",
    "{\\bf x} \\mapsto \\sum_{i=1}^{d}\\cos(x_i)\n",
    "\\end{cases}\\,.\n",
    "$$\n",
    "The objective is to estimate the function $f$ using a training data set to predict the observations associated with the test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tY1C93KOKwCH"
   },
   "source": [
    "Start with a few imports, in particular the function ``RandomForestRegressor`` of sklearn that enables to apply the random forest algorithm in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "p-cHmWx2KwCH"
   },
   "outputs": [],
   "source": [
    "# ignore warnings for better clarity (may not be the best thing to do)...\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J3IyqegyKwCI"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "%matplotlib inline\n",
    "\n",
    "import requests\n",
    "\n",
    "exec(requests.get(\"https://raw.githubusercontent.com/claireBoyer/tutorial-conformal-prediction/main/labs/aux-npt/get-send-code.html\").content)\n",
    "\n",
    "npt_config = {\n",
    "    'session_name': 'Random-Forests',\n",
    "    'session_owner': 'Aymeric',\n",
    "    'sender_name': input(\"Your name:\"),\n",
    "}\n",
    "send('started',1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mrEOmkL3KwCI"
   },
   "source": [
    "<font color=blue> The following cell allows me to track your progress and see some of your results. You can use your first name or a  pseudonym if you prefer! <br>\n",
    "When you see a \"send\" function, just ignore it !\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cpW764AZKwCJ"
   },
   "source": [
    "### <font color=darkorange> For a few cells, you have to complete a few lines of code. The solution is always given in the cell just below.</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_75N9SsEKwCJ"
   },
   "source": [
    "In the following cell, we define a function that takes as input the size $d$ of the vector $x_0$, the size $n$ of the dataset, the noise levels $\\sigma_X$ of the autoregressive model and $\\sigma_Y$ of the observed values and the parameter $\\rho$ in the autoregressive model.\n",
    "\n",
    "The function outputs data following the model described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "TxCRInCDKwCJ"
   },
   "outputs": [],
   "source": [
    "# function to sample a dataset \n",
    "def sample_data_ar(rho,sigmax,sigmay,n,d):\n",
    "    X       = np.zeros(shape=(n,d))\n",
    "    Y       = np.zeros(n)\n",
    "    epsilon = np.random.normal(loc=0,scale=1,size = X.shape)\n",
    "    eta     = np.random.normal(loc=0,scale=1,size = n)\n",
    "    Y[0]    = np.sum(np.cos(X[0,:])) + sigmax*eta[0]\n",
    "    for k in range(1,n):\n",
    "        X[k,:] = (rho*X[k-1,:] + sigmax*epsilon[k,:])%(2*np.pi)\n",
    "        Y[k]   = np.sum(np.cos(X[k,:])) + sigmay*eta[k]\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "XsSKPZIwKwCJ"
   },
   "outputs": [],
   "source": [
    "# set parameters\n",
    "n = 2000\n",
    "d = 2\n",
    "\n",
    "rho    = 0.8\n",
    "sigmax = 0.1 \n",
    "sigmay = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hiqUOqtYKwCK"
   },
   "source": [
    "Matrices X and Y containing data simulated according to the observation model described above may now be computed.\n",
    "\n",
    "In this simulated toy model, the values of the hidden Markov chain $X_k$ are available in the matrix $X$. \n",
    "In practice, only the vector $Y$ is observed (the values of $X_k$ must be estimated sequentially)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "txfMGl8TKwCK"
   },
   "source": [
    "<strong><font color=darkorange> Q1</font></strong>: Generate the data X,Y using the ```sample_data_ar``` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "6o9KGtRrKwCK"
   },
   "outputs": [],
   "source": [
    "# sample data  \n",
    "# Complete here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZXsB8fu6KwCK"
   },
   "source": [
    "<strong><font color=darkorange> Solution 1</font></strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "D57hCoyGKwCL"
   },
   "outputs": [],
   "source": [
    "# sample data  \n",
    "X, Y = sample_data_ar(rho,sigmax,sigmay,n,d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Yd11VOEKwCL"
   },
   "source": [
    "To evaluate different algorithms, the dataset is decomposed into training and test data.\n",
    "In this type of time series analysis, the training data are the first values of $Y_k$ and the test are the last one. This corresponds to situations where one wants to predict future values of $Y$ given historical data.\n",
    "This is somehow different form the i.i.d. case where taining and test data are chosen randomly using the function ``sklearn.model_selection.train_test_split``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "kGxffJJrKwCL"
   },
   "outputs": [],
   "source": [
    "# split variables and observations, using 90% of the data set to estimate f \n",
    "import pandas as pd\n",
    "if d==4:\n",
    "    df = pd.DataFrame(data = X, columns = ['X1', 'X2', 'X3', 'X4'])\n",
    "if d==2:\n",
    "    df = pd.DataFrame(data = X, columns = ['X1', 'X2'])\n",
    "df['Y'] = Y\n",
    "\n",
    "nb_data_train = int(0.95*n)\n",
    "nb_diff       = n-nb_data_train\n",
    "df.head()\n",
    "X_train = df.iloc[0:nb_data_train,:-1] \n",
    "X_test  = df.iloc[-nb_diff:,0:-1]\n",
    "X_train.head()\n",
    "Y_train = df.iloc[0:nb_data_train,-1] \n",
    "Y_test  = df.iloc[-nb_diff:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XIq1RTYqKwCL"
   },
   "source": [
    "We can visualize the data in the following graph, with the colors corresponding to the value of Y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DYhynXN8KwCM"
   },
   "source": [
    "<strong><font color=darkorange> Q2</font></strong>: Generate a scatter plot of the first two coordinates of X,  using the value of Y to fix the colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "h6ZnMUoiKwCM"
   },
   "outputs": [],
   "source": [
    "#Complete here\n",
    "\n",
    "#Keep\n",
    "send(plt, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WsLAjMjbKwCM"
   },
   "source": [
    "<strong><font color=darkorange> Solution 2</font></strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "MyTx12bOKwCM"
   },
   "outputs": [],
   "source": [
    "plt.scatter(X[:,0],X[:,1], c=Y, cmap=plt.cm.RdYlBu)\n",
    "plt.colorbar()\n",
    "send(plt, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VhyWuMEKKwCN"
   },
   "source": [
    "<strong><font color=darkorange> Q3</font></strong>: change the parameters rho to 0.1 et re-run the cells, to see the impact on the distribution of X."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5t6IxmOJKwCN"
   },
   "source": [
    "### A first Tree regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kS_hvi6WKwCN"
   },
   "source": [
    "At first, we can perform regression using a Regression Tree:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WQPKOV-CKwCN"
   },
   "source": [
    "<strong><font color=darkorange> Q4</font></strong>: In the following cell, we focus on the two most important steps with sklearn:\n",
    "- defining the model. Chose the depth of the tree to be 2 (you can also try different values) \n",
    "- fitting the model\n",
    "Complete the two lines needed to do so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "RLv6jNXeKwCO"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor, plot_tree, export_graphviz\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "clf = #COMPLETE HERE !!!\n",
    "clf.fit(#COMPLETE HERE)!!!\n",
    "\n",
    "plot_tree(clf, filled=True)\n",
    "send(plt, 4)\n",
    "plt.show()\n",
    "\n",
    "    \n",
    "import graphviz \n",
    "dot_data = export_graphviz(clf, filled=True, out_file=None) \n",
    "graph = graphviz.Source(dot_data) \n",
    "graph.render(\"syntehtic_data_dec_tree\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "kVJRdRdoKwCO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "WQzxu6hSKwCO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6D81dpV2KwCO"
   },
   "source": [
    "<strong><font color=darkorange> Solution 4</font></strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "UhzMmKRrKwCO"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor, plot_tree, export_graphviz\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "clf = DecisionTreeRegressor(max_depth=2, random_state=0)\n",
    "clf.fit(X_train.values, Y_train.values)\n",
    "\n",
    "plot_tree(clf, filled=True)\n",
    "send(plt, 5)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "import graphviz \n",
    "dot_data = export_graphviz(clf, filled=True, out_file=None) \n",
    "graph = graphviz.Source(dot_data) \n",
    "graph.render(\"syntehtic_data_dec_tree\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YtYG1PltKwCO"
   },
   "source": [
    "We can visualize the frontiers on the first 2 dimensions using the ```contourf``` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "pyDAyllvKwCP",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if d==2:\n",
    "    plot_step = 0.02\n",
    "    x_min, x_max = 0, 2*np.pi\n",
    "    y_min, y_max = 0, 2*np.pi\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n",
    "                         np.arange(y_min, y_max, plot_step))\n",
    "    plt.tight_layout(h_pad=0.5, w_pad=0.5, pad=2.5)\n",
    "\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    cs = plt.contourf(xx, yy, Z, cmap=plt.cm.RdYlBu)\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X5bLbzFaKwCP"
   },
   "source": [
    "<strong><font color=darkorange> Q5</font></strong>:\n",
    "- Is this tree coherent with your intuition looking at the scatter plot?\n",
    "- Do you think the classification accuracy is good enough (are there enough details?)\n",
    "- Modify the following cell to have deeper trees (depths =5, 10 for ex). What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "DmpWSlvjKwCP"
   },
   "outputs": [],
   "source": [
    "if d==2:\n",
    "    clf = DecisionTreeRegressor(#TOCOMPLETE, random_state=0)\n",
    "    clf.fit(X_train.values, Y_train.values)\n",
    "\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    cs = plt.contourf(xx, yy, Z, cmap=plt.cm.RdYlBu)\n",
    "    plt.colorbar()\n",
    "    send(plt,6)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "2r2R4d6FKwCP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "wHrvDrvZKwCP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "auJR9SQOKwCQ"
   },
   "source": [
    "<strong><font color=darkorange> Sol 5</font></strong>:\n",
    "- The tree is coherent but not good enough\n",
    "- We have a much better regression function for a higher depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "PmlpVQUoKwCQ"
   },
   "outputs": [],
   "source": [
    "if d==2:\n",
    "    clf = DecisionTreeRegressor(max_depth=5, random_state=0)\n",
    "    clf.fit(X_train.values, Y_train.values)\n",
    "\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    cs = plt.contourf(xx, yy, Z, cmap=plt.cm.RdYlBu)\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iHY75Rd8KwCQ"
   },
   "source": [
    "### Regression Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SGEn2smnKwCQ"
   },
   "source": [
    "A first random forest prediction can be performed. \n",
    "``rf`` is the random forest function of sklearn when the number of trees in the forest is set to ``n\\_trees``.\n",
    "The forest is estimated using the training data ``X\\_train`` and ``Y\\_train`` and the values of $Y$ in the test set are compared to those predicted by the algorithm.\n",
    "The mean-squared error between these quantities is also displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "g9-ySBWpKwCQ"
   },
   "outputs": [],
   "source": [
    "# We change parmeters for the following, incresing dimension to 4\n",
    "# set parameters\n",
    "n = 2000\n",
    "d = 2\n",
    "\n",
    "rho    = 0.9\n",
    "sigmax = 0.1 \n",
    "sigmay = 0.2\n",
    "\n",
    "# sample data  \n",
    "X, Y = sample_data_ar(rho,sigmax,sigmay,n,d)\n",
    "\n",
    "# split variables and observations, using 90% of the data set to estimate f \n",
    "if d==4:\n",
    "    df = pd.DataFrame(data = X, columns = ['X1', 'X2', 'X3', 'X4'])\n",
    "if d==2:\n",
    "    df = pd.DataFrame(data = X, columns = ['X1', 'X2'])\n",
    "df['Y'] = Y\n",
    "\n",
    "nb_data_train = int(0.95*n)\n",
    "nb_diff       = n-nb_data_train\n",
    "df.head()\n",
    "X_train = df.iloc[0:nb_data_train,:-1] \n",
    "X_test  = df.iloc[-nb_diff:,0:-1]\n",
    "X_train.head()\n",
    "Y_train = df.iloc[0:nb_data_train,-1] \n",
    "Y_test  = df.iloc[-nb_diff:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eVmzW5SWKwCR"
   },
   "source": [
    "<strong><font color=darkorange> Q5</font></strong>:\n",
    "In the following cell: \n",
    "- define the regressor (a ```RandomForestRegressor``` with ```n_trees``` number of trees\n",
    "- fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "4YzJD0GmKwCR",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# number of trees in the forest for an elementary random forest estimate\n",
    "n_trees = 100\n",
    "\n",
    "\n",
    "rf      = #COMPLETEHERE\n",
    "#COMPLETE HERE TO FIT THE MODEL\n",
    "\n",
    "\n",
    "\n",
    "# compute predictions usting test data and associated mean square error\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "mse       = mean_squared_error(Y_test,y_pred_rf)\n",
    "print('mse is', \"%.2e\" %mse)\n",
    "\n",
    "plt.figure(1,figsize=(8,8))\n",
    "plt.title('Dimension of the state space: %d, MSE %f' %(d,mse))\n",
    "plt.scatter(Y_test,y_pred_rf, color='b', s = 10, alpha = 0.8)\n",
    "plt.plot(Y_test,Y_test,color='k', linestyle = 'dashed')\n",
    "send(plt,7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "fRpRqMUjKwCR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "dFcAyyEAKwCR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qi4OfMg2KwCR"
   },
   "source": [
    "<strong><font color=darkorange> Solution 5</font></strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "uN76xYx_KwCS"
   },
   "outputs": [],
   "source": [
    "# number of trees in the forest for an elementary random forest estimate\n",
    "n_trees = 100\n",
    "rf      = RandomForestRegressor(n_estimators = n_trees)\n",
    "rf.fit(X_train,Y_train)\n",
    "\n",
    "# compute predictions usting test data and associated mean square error\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "mse       = mean_squared_error(Y_test,y_pred_rf)\n",
    "print('mse is', \"%.2e\" %mse)\n",
    "\n",
    "plt.figure(1,figsize=(8,8))\n",
    "plt.title('Dimension of the state space: %d, MSE %f' %(d,mse))\n",
    "plt.scatter(Y_test,y_pred_rf, color='b', s = 10, alpha = 0.8)\n",
    "plt.plot(Y_test,Y_test,color='k', linestyle = 'dashed')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "xg401TYjLHZ1"
   },
   "outputs": [],
   "source": [
    "Z = rf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "cs = plt.contourf(xx, yy, Z, cmap=plt.cm.RdYlBu)\n",
    "plt.colorbar()\n",
    "send(plt,6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OMVwOC7dKwCS"
   },
   "source": [
    "The predictions seem reasonable but it may be that these predictions are very accurate at first and then that the errors accumulate over time.\n",
    "To test if this is the case, plot the values of $Y$ in the test set (true and predicted) as functions of the time from the last training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "Vi7GgsC2KwCS"
   },
   "outputs": [],
   "source": [
    "plt.figure(1,figsize=(8,8))\n",
    "plt.plot(Y_test.values,marker='o',color='k')\n",
    "plt.plot(y_pred_rf,'g--',marker='+')\n",
    "plt.legend(labels=('Future values','Random forest estimates'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DDFAwKGnKwCS"
   },
   "source": [
    "To improve this first prediction, the different parameters of the random forest algorithm may be tuned. For example, an increase of the number of trees in the forest reduced the variance of the Monte Carlo estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "MGtCckIeKwCS"
   },
   "outputs": [],
   "source": [
    "# number of trees in the forest\n",
    "n_trees = 500\n",
    "rf      = RandomForestRegressor(n_estimators = n_trees)\n",
    "rf.fit(X_train,Y_train)\n",
    "\n",
    "# compute predictions usting test data and associated mean square error\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "mse       = mean_squared_error(Y_test,y_pred_rf)\n",
    "\n",
    "plt.figure(1,figsize=(8,8))\n",
    "plt.title('Dimension of the state space: %d, MSE %f' %(d,mse))\n",
    "plt.scatter(Y_test,y_pred_rf, color='b', s = 10, alpha = 0.8)\n",
    "plt.plot(Y_test,Y_test,color='k', linestyle = 'dashed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3FbEX421KwCS"
   },
   "source": [
    "Note that this increases substancially the computational time although predictions seem equally relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "A8M8tRA-KwCT"
   },
   "outputs": [],
   "source": [
    "plt.figure(1,figsize=(8,8))\n",
    "plt.plot(Y_test.values,marker='o',color='k')\n",
    "plt.plot(y_pred_rf,'g--',marker='+')\n",
    "plt.legend(labels=('Future values','Random forest estimates'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d6DLTfjyKwCT"
   },
   "source": [
    "<strong><font color=red> You can skip the next few cells to work directly on the Brazil dataset </font></strong>\n",
    "\n",
    "The objective is now to improve the algorithm using a cross-validation scheme to estimate the best value of these parameters. It is useful to look at the parameters used by default in the algorithm here:\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\n",
    "\n",
    "Set a grid of parameters that will be tested by the cross-validation procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "wbSwAXsnKwCT"
   },
   "outputs": [],
   "source": [
    "# with RandomizedSearchCVCreate, a grid of tuning parameters is built\n",
    "# then a random search will be performed to test which parameter values yield the best \n",
    "# random forest estimate (see below).\n",
    "from pprint import pprint\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# Number of trees in the forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# max_features is the number of dimension considered to select the best split (the dimension along which a cell is cut)\n",
    "max_features = ['log2', 'sqrt']\n",
    "# The maximum depth of the tree corresponds to the maximum number of levels of the tree. \n",
    "# If not given, splits are performed until all cells contain less than min_samples_split samples.\n",
    "max_depth = np.arange(5,100,5)\n",
    "# min_samples_split is the minimum number of samples in a cell to allow a split.\n",
    "min_samples_split = [2, 3, 4, 5, 6, 7, 8]\n",
    "# A split is considered in the tree if it leaves at least min_samples_leaf training samples in each \n",
    "# subcell obtained after the spliting process.\n",
    "min_samples_leaf = [1, 2, 3, 4]\n",
    "# if bootstrap is true all the training dataset is used to build each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "pprint(random_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tl3R2V8CKwCT"
   },
   "source": [
    "Exploring all parameters combinations in this grid would be computationally prohibitive. \n",
    "An efficient alternative is to compare parameters chosen at random in the grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "yec9YBRgKwCT"
   },
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor()\n",
    "# Random search among  all parameters (the number of possible combination is given. It is not a complete grid search !!!).\n",
    "# search across n_iter = 20 different combinations with a default 3-fold cross validation.\n",
    "rf_random = RandomizedSearchCV(estimator=rf,\n",
    "                               param_distributions=random_grid,\n",
    "                               n_iter=20,\n",
    "                               cv=3,\n",
    "                               verbose=10,\n",
    "                               n_jobs=-1)\n",
    "\n",
    "rf_random.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "8MvQH6_CKwCU"
   },
   "outputs": [],
   "source": [
    "# display the best parameters\n",
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "qFaXzOhmKwCU"
   },
   "outputs": [],
   "source": [
    "# build the best explored random forest\n",
    "best_random = rf_random.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "NacPiVRoKwCU"
   },
   "outputs": [],
   "source": [
    "plt.figure(1,figsize=(8,8))\n",
    "plt.title('Dimension of the state space: %d, MSE %f' %(d,mse))\n",
    "plt.scatter(Y_test,y_pred_rf, color='b', s = 10, alpha = 0.8)\n",
    "plt.plot(Y_test,Y_test,color='k', linestyle = 'dashed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "nVQqTApGKwCU"
   },
   "outputs": [],
   "source": [
    "# compute predictions usting test data and associated mean square error\n",
    "y_pred_rf = rf_random.predict(X_test)\n",
    "mse       = mean_squared_error(Y_test,y_pred_rf)\n",
    "plt.figure(1,figsize=(8,8))\n",
    "plt.plot(Y_test.values,marker='o',color='k')\n",
    "plt.plot(y_pred_rf,'g--',marker='+')\n",
    "plt.legend(labels=('Future values','Random forest estimates'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5M3LWZcAKwCU"
   },
   "source": [
    "## <font color=darkred> Random forests for time series prediction - Inflation prediction </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "TKv64-77KwCU"
   },
   "outputs": [],
   "source": [
    "! wget -q https://raw.githubusercontent.com/adieulev/Certificate-DS-2021/main/Data/BRinf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3d4WsztWKwCV"
   },
   "source": [
    "<strong><font color=darkorange> Q7</font></strong>:\n",
    "In the following cell, we import the data on a pandas dataframe. Print the first few lines (use head), the shape of  ```df```.\n",
    "How many feature are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "uxTM0RdaKwCV"
   },
   "outputs": [],
   "source": [
    "# In this section, random forests are used to predic the Brazilian inflation based on\n",
    "# many observed variables, see https://github.com/gabrielrvsc/HDeconometrics/\n",
    "import pandas as pd\n",
    "df = pd.read_csv('BRinf')\n",
    "\n",
    "##### COMPLETE HERE to print first lines\n",
    "##### COMPLETE HERE TO PRINT SHAPE\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "tEBJnCbNKwCV"
   },
   "outputs": [],
   "source": [
    "# COMPLETE the ??\n",
    "send(\"there are ??? observations and ?? feature\", 8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ierE61eeKwCV"
   },
   "source": [
    "<strong><font color=darkorange> Solution 7</font></strong>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "P65PM_f7KwCV"
   },
   "outputs": [],
   "source": [
    "# In this section, random forests are used to predic the Brazilian inflation based on\n",
    "# many observed variables, see https://github.com/gabrielrvsc/HDeconometrics/\n",
    "import pandas as pd\n",
    "df = pd.read_csv('BRinf')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "I9hxab50KwCV",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# number of observations, number of variables\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dLVL9MnhKwCV"
   },
   "source": [
    "The function ``pandas.DataFrame.corr`` may be used to compute the pairwise correlations between columns (variables and inflation). These correlations can be displayed using ``sns.heatmap`` to highlight highly correlated variables (and those likely to have an impact on the inflation).\n",
    "\n",
    "See ``feature_importances_`` below for a first try at (relevant) variable selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "GzlB1TTDKwCW"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "corr = df.corr()\n",
    "fig, ax = plt.subplots(figsize=(10,10))  \n",
    "sns.heatmap(corr, xticklabels = False, yticklabels = False, cmap = 'Blues', ax = ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gY-257n_KwCW"
   },
   "source": [
    "<strong><font color=darkorange> Q8</font></strong>: \n",
    "- which column are we trying to predict?\n",
    "- which features have the highest correlation with it?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "bQOsOGl_KwCW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cYoAKRR5KwCW"
   },
   "source": [
    "<strong><font color=darkorange> Solution 8</font></strong>: \n",
    "- ```np.argmax(corr.values[0,1:]''' finds the index of the highest correlationon the column 0 (ignoring the first element which has cor 1!)\n",
    "- we add 2: one for the first element, one for the first empty column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "__zwiz2NKwCW"
   },
   "outputs": [],
   "source": [
    "df.columns[np.argmax(corr.values[0,1:])+2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "Vl8p1tWSKwCW"
   },
   "outputs": [],
   "source": [
    "corr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "NcTzOL0NKwCX",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nb_data_train = 140\n",
    "nb_diff       = df.shape[0]-nb_data_train\n",
    "# inflation observations\n",
    "Y_train = df.iloc[0:nb_data_train,1] \n",
    "Y_test  = df.iloc[-nb_diff:,1] \n",
    "Y_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "bBLE29DHKwCX"
   },
   "outputs": [],
   "source": [
    "# other variables\n",
    "X_train = df.iloc[0:nb_data_train,2:] \n",
    "X_test  = df.iloc[-nb_diff:,2:] \n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "nZlu2M_pKwCX",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_trees = 100\n",
    "rf      = RandomForestRegressor(n_estimators = n_trees)\n",
    "rf.fit(X_train,Y_train)\n",
    "\n",
    "# compute predictions usting test data and associated mean square error\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "\n",
    "plt.figure(1,figsize=(8,8))\n",
    "plt.plot(Y_test.values,marker='o',color='k')\n",
    "plt.plot(y_pred_rf,'g--',marker='+')\n",
    "plt.legend(labels=('Future inflation values','Random forest estimates'))\n",
    "send(plt,9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "C_j0OV-lKwCX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gBaq3SUlKwCX"
   },
   "source": [
    "<strong><font color=darkorange> Q9</font></strong>: \n",
    "- What do you think of the quality of the prediction?\n",
    "- Do you think it is useful to use all explanatory variables?\n",
    "- Which variables have the most importance? What was the method proposed in the lecture to chose variables?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "Hufb3PiCKwCY"
   },
   "outputs": [],
   "source": [
    "### COMPLETE the ??\n",
    "\n",
    "send(\"the prediction is ???\", 10)\n",
    "send(\"we could use ??more/less?? features\", 11 ) # chose more or less, guess how many would be useful...\n",
    "send(\"we can use the ???? function in sklearn\", 12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "R_CLVugzKwCY"
   },
   "source": [
    "<strong><font color=darkorange> Solution 9</font></strong>: \n",
    "- Many variables are used for the inflation prediction while very few observations are available.\n",
    "- Selecting the most valuable variables is an alternativ to learn a simpler models\n",
    "- This is obtained in Python with rf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "MSoo1FYtKwCY",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(1,figsize=(12,12))\n",
    "plt.bar(list(df)[2:93],rf.feature_importances_,align='center')\n",
    "plt.xticks(range(len(list(df)[2:93])),list(df)[2:93],rotation=90,size='small')\n",
    "plt.title('Feature importances')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "50EZmMyJKwCY"
   },
   "source": [
    "RandomForestClassifier and RandomForestRegressor use the gini importance mechanism as a measure of the \n",
    "fetaures importance. The mean decrease in impurity importance of a \n",
    "feature is computed by measuring the impact of a variable on the variance of the prediction,\n",
    "see https://papers.nips.cc/paper/4928-understanding-variable-importances-in-forests-of-randomized-trees.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "LxZDa2R1KwCY",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# collect the indices of the features with highest importance.\n",
    "nb_features_to_keep = 10\n",
    "ind = rf.feature_importances_.argsort()[-nb_features_to_keep:]\n",
    "X_train.iloc[:,ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "9jZ10t6gKwCY"
   },
   "outputs": [],
   "source": [
    "n_trees = 100\n",
    "rf      = RandomForestRegressor(n_estimators = n_trees)\n",
    "rf.fit(X_train.iloc[:,ind],Y_train)\n",
    "\n",
    "# compute predictions usting test data and associated mean square error\n",
    "y_pred_rf = rf.predict(X_test.iloc[:,ind])\n",
    "\n",
    "plt.figure(1,figsize=(8,8))\n",
    "plt.plot(Y_test.values,marker='o',color='k')\n",
    "plt.plot(y_pred_rf,'g--',marker='+')\n",
    "plt.legend(labels=('Future inflation values','Random forest estimates'))\n",
    "send(plt,13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2JwRvmLaKwCZ"
   },
   "source": [
    "<strong><font color=darkorange> Q10</font></strong>: \n",
    "- What do you think of the quality of the prediction?\n",
    "- How many explanatory variables are we using now?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "Jkp506qHKwCZ"
   },
   "outputs": [],
   "source": [
    "# COMPLETE the ??\n",
    "\n",
    "send(\"the predictions are ??better/worse??\", 14)\n",
    "send(\"we are using ?? explanatory variables\", 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MDeWqIbwKwCZ"
   },
   "source": [
    "<strong> We now plot the MSE as a function of the numbers of variables used to estimate the function f</strong> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "Gm5Sy4ZMKwCZ"
   },
   "outputs": [],
   "source": [
    "d_max = 60\n",
    "MSE   = []\n",
    "rf    = RandomForestRegressor(n_estimators = n_trees)\n",
    "rf.fit(X_train,Y_train)\n",
    "for d in range(2,d_max): \n",
    "    rfd = RandomForestRegressor(n_estimators = n_trees)\n",
    "    ind = rf.feature_importances_.argsort()[-d:]\n",
    "    rfd.fit(X_train.iloc[:,ind],Y_train)\n",
    "    # compute predictions usting test data and associated mean square error\n",
    "    y_pred_rf = rfd.predict(X_test.iloc[:,ind])\n",
    "    MSE = np.append(MSE,mean_squared_error(Y_test,y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "_oHyTmUSKwCZ"
   },
   "outputs": [],
   "source": [
    "#plt.figure(1,figsize=(8,8))\n",
    "plt.plot(MSE,marker='o',color='k')\n",
    "plt.xlabel('Number of features')\n",
    "plt.ylabel('Mean square error on the test data set')\n",
    "send(plt,16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "jSpVokxtKwCZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aK6Q5JrbKwCZ"
   },
   "source": [
    "<strong><font color=darkorange> Q11</font></strong>: \n",
    "- Which extremely important phenomenon are we observing here? \n",
    "- What is the optimal number of explanatory variables?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "I9VXNXxmKwCa"
   },
   "outputs": [],
   "source": [
    "# COMPLETE the ??\n",
    "\n",
    "send(\"We observe ????\", 16)\n",
    "send(\"we should use the optimam number of ???? explanatory variables\", 17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vIb40GL7KwCa"
   },
   "source": [
    "<strong><font color=darkorange> Q12</font></strong>: \n",
    "- Picking manually the optimal number of features, fit the model and print the predictions !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "KZL9NxeyKwCa"
   },
   "outputs": [],
   "source": [
    "# collect the indices of the features with highest importance.\n",
    "\n",
    "nb_features_to_keep = #COMPLETE\n",
    "ind = rf.feature_importances_.argsort()[-nb_features_to_keep:]\n",
    "\n",
    "n_trees = # COMPLETE\n",
    "\n",
    "rf      = # COMPLETE\n",
    "#COMPLETE\n",
    "\n",
    "# compute predictions usting test data and associated mean square error\n",
    "y_pred_rf = rf.predict(#COMPLETE)\n",
    "\n",
    "plt.figure(1,figsize=(8,8))\n",
    "plt.plot(Y_test.values,marker='o',color='k')\n",
    "plt.plot(y_pred_rf,'g--',marker='+')\n",
    "send(plt,18)\n",
    "plt.legend(labels=('Future inflation values','Random forest estimates'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8DMQKZPMKwCa"
   },
   "source": [
    "<strong><font color=darkorange> Solution 11 & 12</font></strong>: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "7E8rvD9NKwCa"
   },
   "outputs": [],
   "source": [
    "# collect the indices of the features with highest importance.\n",
    "nb_features_to_keep = 4\n",
    "ind = rf.feature_importances_.argsort()[-nb_features_to_keep:]\n",
    "X_train.iloc[:,ind]\n",
    "n_trees = 100\n",
    "rf      = RandomForestRegressor(n_estimators = n_trees)\n",
    "rf.fit(X_train.iloc[:,ind],Y_train)\n",
    "\n",
    "# compute predictions usting test data and associated mean square error\n",
    "y_pred_rf = rf.predict(X_test.iloc[:,ind])\n",
    "\n",
    "plt.figure(1,figsize=(8,8))\n",
    "plt.plot(Y_test.values,marker='o',color='k')\n",
    "plt.plot(y_pred_rf,'g--',marker='+')\n",
    "plt.legend(labels=('Future inflation values','Random forest estimates'))\n",
    "send(plt,19)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jdsbS-0CKwCa"
   },
   "source": [
    "<strong><font color=darkorange>\n",
    "- Using too many features results in overfitting: the accuracy on the test set becomes worse\n",
    "- The optimal predictor uses only the four most important features</font></strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "Ji5SRJcIKwCb"
   },
   "outputs": [],
   "source": [
    "# you may add here a cross validation procedure using RandomizedSearchCV or GridSearchCV\n",
    "# to select the best parameters (and compare the mean square error with what you obtain above)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "Lab2021-RF-withQ.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

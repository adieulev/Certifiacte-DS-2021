{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=darkcyan>  Machine Learning in Python </font>\n",
    "\n",
    "### <font color=darkorange> Random Forests & Neural networks</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. <font color=darkred> Handwritten digit recognition with MNIST </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "This section aims at using neural networks to perform classification on the [MNIST](http://yann.lecun.com/exdb/mnist) dataset.\n",
    "This dataset contains images representing handwritten digits.  Each image is made of 28 x 28 pixels, and each pixel is represented by an integer (gray level).  These arrays can be flattened into vectors of 28 x 28 = 784 numbers.\n",
    "Each image is then a 784-dimensional vector.  Visualisations of this vector space are given here: [http://colah.github.io/posts/2014-10-Visualizing-MNIST/](http://colah.github.io/posts/2014-10-Visualizing-MNIST/).\n",
    "\n",
    "The labels in $\\{0, 1, 2, \\ldots, 9\\}$ can represented using one-hot encoding: labels in $\\{0, 1, 2, \\ldots, 9\\}$ are replaced by labels in $\\{ 0, 1\\}^{10}$, namely $0$ is replaced by $(1, 0, \\ldots 0)$, $1$ is replaced by $(0, 1, 0, \\ldots 0)$, $2$ is replaced by $(0, 0, 1, 0, \\ldots, 0)$, etc.\n",
    "\n",
    "Also, MNIST data is grayscale pixels in $\\{0, \\ldots, 255\\}$ which should be normalized to belong to $[0, 1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue> The following cell allows me to track your progress and see some of your results. You can use your first name or a  pseudonym if you prefer! <br>\n",
    "When you see a \"send\" function, just ignore it !\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "exec(requests.get(\"https://courdier.pythonanywhere.com/get-send-code\").content)\n",
    "\n",
    "npt_config = {\n",
    "    'session_name': 'Deep Learning Mnist',\n",
    "    'session_owner': 'Aymeric',\n",
    "    'sender_name': input(\"Your name:\"),\n",
    "}\n",
    "send('started', 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=darkorange> For a few cells, you have to complete a few lines of code. The solution is always given in the cell just below.</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "#from keras import backend as K\n",
    "# Number of classes\n",
    "num_classes = 10\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "\n",
    "x_train     = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "x_test      = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test  = x_test.astype('float32')\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)\n",
    "\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 2))\n",
    "for i in range(4):\n",
    "    plt.subplot(1, 4, i+1)\n",
    "    plt.imshow(x_train[i].reshape(28, 28), \n",
    "               interpolation=\"none\", cmap=\"gray_r\")\n",
    "    plt.title('Label = %d' % y_train[i], fontsize=14)\n",
    "    plt.axis(\"off\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_rows = 4\n",
    "n_cols = 8\n",
    "plt.figure(figsize=(8, 4))\n",
    "for i in range(n_rows * n_cols):\n",
    "    plt.subplot(n_rows, n_cols, i+1)\n",
    "    plt.imshow(x_train[i].reshape(28, 28),\n",
    "               interpolation=\"none\", cmap=\"gray_r\")\n",
    "    plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "send(plt, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(np.array2string(x_train[0].astype(np.int).reshape(28, 28),max_line_width=150))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong><font color=darkorange> Q1</font></strong>: To what do the numbers above correspond ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# COMPLETE HERE\n",
    "send(\"the numbers in the matrix correspond to ???? . The digit is a ????\", 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now renormalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print(x_train.min(axis=None), x_train.max(axis=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import activations\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "y_train[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=darkred> Softmax regression </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim is to predict the digit $k$ represented by each picture, for $k \\in \\{0, \\ldots, 9\\}$.\n",
    "\n",
    "Softmax regression provides a model for the probability that an input image $x$ is associated with each class using a simple linear model.  It is assumed that the probability to belong to the class $k$ can be expressed by a weigthed sum of the pixel intensities, with weights $W_{k, 1}, \\ldots, W_{k, 784}$ and plus a bias $b_k$ capturing some variability independent of the input:\n",
    "$$\n",
    "\\text{score}_k(x_i) = \\sum_{j=1}^{784} W_{k, j} x_j + b_k,\n",
    "$$\n",
    "These scores are sometimes called the \"logits\" in the deep learning community.\n",
    "The softmax function is then used to convert these scores into probabilities $p_k$:\n",
    "$$\n",
    "p_k(x_i) = \\text{softmax}(\\text{score}_k(x_i)) = \\frac{\\exp(\\text{score}_k(x_i))}{\\sum_{k'=1}^{10}\\exp(\\text{score}_{k'}(x_i))}\n",
    "$$\n",
    "for $k=1, \\ldots, 10$.\n",
    "\n",
    "To estimatethe model weights $W_{k, j}$ and $b_k$ for $k=1, \\ldots, 10$ and $j=1, \\ldots, 784$, the loss function is given by the negative log-likelihood (see also the section on gradient based method). The negative log-likelihood of a sample with input $x_i \\in \\mathbb R^{784}$ and label $y_iÂ \\in \\{0, 1\\}^{10}$, is given by the cross-entropy between the scores $p_k(x_i)$ and the label $y_i$:\n",
    "$$\n",
    "- \\sum_{k=1}^{10} y_{i, k} \\log(p_k(x_i))\n",
    "$$\n",
    "Stochastic gradient descent can be used to minimize this loss over small batches of data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    m = x.max()\n",
    "    e = np.exp(x - m)\n",
    "    return e / e.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong><font color=darkorange> Q2</font></strong>: Define the logistic softmax model below by adding a single dense layer with num_classes outputs and a softmax activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a model prone to add layers sequentially\n",
    "model = Sequential()\n",
    "\n",
    "# flatten the data replaces 28 * 28 matrices by a 784 dimensional vector\n",
    "# This is always necessary before a fully-connected layer (Dense object)\n",
    "model.add(Flatten(input_shape=input_shape, name='flatten'))\n",
    "\n",
    "# add one dense (fully connected layer) with softmax activation function\n",
    "# As it is the first layer, the input size is mandatory\n",
    "\n",
    "\n",
    "model.add(##### COMPLETE HERE)\n",
    "\n",
    "    \n",
    "    \n",
    "# \"compile\" this model, \n",
    "model.compile(\n",
    "    # specify the loss as the cross-entropy\n",
    "    loss=keras.losses.categorical_crossentropy,\n",
    "    # choose the gradient based method to estimate the parameters\n",
    "    # see https://keras.io/optimizers/ to have an overview of the different options\n",
    "    # see also section 2 on gradient based methods.\n",
    "    optimizer=keras.optimizers.Adagrad(),\n",
    "    # metric to monitor on the test data\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "send(\"model\", 3)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong><font color=darkorange> Solution 2</font></strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a model prone to add layers sequentially\n",
    "\n",
    "model = Sequential()\n",
    "# flatten the data replaces 28 * 28 matrices by a 784 dimensional vector\n",
    "# This is always necessary before a fully-connected layer (Dense object)\n",
    "model.add(Flatten(input_shape=input_shape, name='flatten'))\n",
    "\n",
    "# add one dense (fully connected layer) with softmax activation function\n",
    "# As it is the first layer, the input size is mandatory\n",
    "model.add(Dense(num_classes, activation='softmax', name='dense_softmax'))\n",
    "\n",
    "# \"compile\" this model, \n",
    "model.compile(\n",
    "    # specify the loss as the cross-entropy\n",
    "    loss=keras.losses.categorical_crossentropy,\n",
    "    # choose the gradient based method to estimate the parameters\n",
    "    # see https://keras.io/optimizers/ to have an overview of the different options\n",
    "    # see also section 2 on gradient based methods.\n",
    "    optimizer=keras.optimizers.Adagrad(),\n",
    "    # metric to monitor on the test data\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "send(\"model\", 4)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 10\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 5))\n",
    "plt.plot(history.epoch, history.history['accuracy'], lw=1, label='Train')\n",
    "plt.plot(history.epoch, history.history['val_accuracy'], lw=1, label='Test')\n",
    "plt.legend(fontsize=14)\n",
    "plt.title('Accuracy of softmax regression', fontsize=16)\n",
    "plt.xlabel('Epoch', fontsize=14)\n",
    "plt.ylabel('Accuracy', fontsize=14)\n",
    "plt.tight_layout()\n",
    "send(plt, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong><font color=darkorange> Q3</font></strong>: What do you think of the accuracy? is it good?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights, biases = model.get_layer('dense_softmax').get_weights()\n",
    "imgs = weights.reshape(28, 28, 10)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "vmin, vmax = imgs.min(), imgs.max()\n",
    "for i in range(10):\n",
    "    ax = plt.subplot(2, 5, i + 1)\n",
    "    im = imgs[:, :, i]\n",
    "    mappable = ax.imshow(im, interpolation=\"nearest\", \n",
    "                         vmin=vmin, vmax=vmax, cmap='bwr')\n",
    "    ax.axis('off')\n",
    "    ax.set_title(\"%i\" % i)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=darkred> Feed-Forward Neural Network (FFNN) </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The softmax regression of the previous section is a linear model, with 7850 parameters.  It is easy to fit, numerically stable, but might be too simple for some tasks.  The idea underlying neural networks is to have successive \"neurons\" performing a linear transformation of the input data (depending on a weight matrix and a bia vector) followed by an activation function to design more flexible models with additional parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong><font color=darkorange> Q4 </font></strong>: Define  a fully connected feed-forward neural network with **one** hidden layer with 128 units and a relu activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_ffnn = Sequential()\n",
    "\n",
    "model_ffnn.add(Flatten(input_shape=input_shape))\n",
    "\n",
    "model_ffnn.add(### COMPLETE HERE)\n",
    "\n",
    "model_ffnn.add(### COMPLETE HERE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong><font color=darkorange> Solution 4 </font></strong>: Define  a fully connected feed-forward neural network with **one** hidden layer with 128 units and a relu activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_ffnn = Sequential()\n",
    "\n",
    "model_ffnn.add(Flatten(input_shape=input_shape))\n",
    "\n",
    "model_ffnn.add(Dense(128, activation='relu'))\n",
    "\n",
    "model_ffnn.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "send('model_ffnn', 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many parameters are involved in this model ?\n",
    "\n",
    "The input size is 28*28 = 724. \n",
    "\n",
    "This input is transformed linearly in 128 hidden units in the dense layer which lead to 128*784 + 128 = 100480 parameters to obtain the 128 units.\n",
    "\n",
    "These units are transformed in a ``softmax`` activation function which adds 10*128 + 10 = 1290 parameters.\n",
    "\n",
    "This Feed Forward Neural Networks depends on 101770 parameters !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_ffnn.compile(\n",
    "    loss=keras.losses.categorical_crossentropy,\n",
    "    optimizer=keras.optimizers.Adagrad(),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model_ffnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 10\n",
    "\n",
    "# Run the train\n",
    "history = model_ffnn.fit(x_train, y_train,\n",
    "                         batch_size=batch_size,\n",
    "                         epochs=epochs,\n",
    "                         verbose=1,\n",
    "                         validation_data=(x_test, y_test))\n",
    "score = model_ffnn.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 5))\n",
    "plt.plot(history.epoch, history.history['accuracy'], lw=3, label='Training')\n",
    "plt.plot(history.epoch, history.history['val_accuracy'], lw=3, label='Testing')\n",
    "plt.legend(fontsize=14)\n",
    "plt.title('Accuracy of NN with fully connected layers', fontsize=16)\n",
    "plt.xlabel('Epoch', fontsize=14)\n",
    "plt.ylabel('Accuracy', fontsize=14)\n",
    "plt.tight_layout()\n",
    "send(plt, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=darkred> Convolutional Neural Network </font>\n",
    "\n",
    "In practice, increasing the size of hidden layers is not very effective. \n",
    "It is often a better idea to add more layers. \n",
    "Intuitively, if the observations have a hierarchical structure, adding more layers can be interpreted as a way to learn more levels of abstraction.  For example, in object detection tasks, it is easier to express shapes from edges and objects from shapes, than to express objects from pixels.  Thus, a good design should try to exploit this hierarchy.\n",
    "\n",
    "In particular cases, such as grid-like data (time series, images), the observations can depend on a pattern which can be associated with different locations of the data. \n",
    "For example, an object can be in the middle or the left of the picture. \n",
    "Thus, the model has to be translation invariant: it is easier to learn how to recognize an object independently of its location. \n",
    "\n",
    "When two inputs might contain the same kind of information, then it is useful to share their weights and estimate the weights jointly for those inputs to learn statistical invariants (things that don't change much on average across time or space). \n",
    "Using this concept on images leads to convolutional neural networks (CNNs), on text, it results on recurrent neural networks (RNNs).  When using CNNs, weights are set to a small kernel that is used to perform a convolution across the image.\n",
    "\n",
    "In the previous example, the step ``model_ffnn.add(Flatten(input_shape=input_shape))`` destroys the spatial organization of the input but is mandatory before the fully connected step: ``model_ffnn.add(Dense(128, activation='relu'))``. The spatial organization of the input may be preserved before using Flatten by convolution steps such as ``model_cnn.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))``. This layer has 32 outputs, each of which depending on a 3*3 weight matrix.\n",
    "\n",
    "Each output neuron of the next layer is computed as follows (image taken from https://adventuresinmachinelearning.com/convolutional-neural-networks-tutorial-tensorflow/) before computing an elementwise RELU function:\n",
    "<img src=\"conv.PNG\">\n",
    "\n",
    "The convolution maps patches of the input image, combined with the convolution kernel, for example\n",
    "$$\n",
    "\\text{output} = \\text{ReLU}(\\text{patch} \\times W + b)\\,.\n",
    "$$\n",
    "\n",
    "Performing the convolution between the image and the weight matrix consists in moving the kernel across the image, and to produce an output for each patch. The way you move across the image is defined by two parameters:\n",
    "\n",
    "- ``Stride``: the stride is the number of pixels you are shifting each time you move your kernel during the convolution.\n",
    "- ``Padding``: defines what happens when the kernel reaches a border of the image when doing the convolution. \n",
    "\"Valid\" padding means that you stop at the edge, while \"Same\" padding allows to go off the edge and pad with zeros so that the width and the height of the output and input tensors are the same.\n",
    "\n",
    "A classical approach is to use a stride of 1 and to combine theoutputs being in some neighborhood. Such an operation combining elements of a tensor is called ``pooling``: ``model_cnn.add(MaxPooling2D(pool_size=(2, 2)))``. Neighborhoods are define by the pooling window dimension (width x height) and the strides you use when moving this window across the image.  Again, a sliding window is applied along the input to produce the output, the most common function applied on each window being the ma function: it aggregates several outputs in a neighborhood $N$ using a max operation: \n",
    "$$\n",
    "\\text{output}'_i = \\max_{j \\in N}\\text{output}_j, \\quad i \\in N.\n",
    "$$\n",
    "The formulas to compute the size of the ouput tensor are the same as for convolution padding and striding: (image taken from https://adventuresinmachinelearning.com/convolutional-neural-networks-tutorial-tensorflow/):\n",
    "<img src=\"maxpool.PNG\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong><font color=darkorange> Q5 </font></strong>: Define  a convolutional neural network as described below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Create the model described below\n",
    "# Convolutional layer with 32 filters and 3 * 3 kernel sizes and 'relu' activation (use the `Conv2D` object)\n",
    "# Convolutional layer with 64 filters and 3 * 3 kernel sizes and 'relu' activation (use the `Conv2D` object)\n",
    "# Max pooling with pool size 2 * 2 (use the `MaxPooling2D` object)\n",
    "# Dropout with probability 0.25 (use the `Dropout` object)\n",
    "# Dense layer with 128 units with relu activation\n",
    "# Dropout with probability 0.5\n",
    "# Dense output layer with softmax activation\n",
    "\n",
    "model_cnn = Sequential()\n",
    "\n",
    "####\n",
    "#TODO\n",
    "####\n",
    "\n",
    "model_cnn.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=keras.optimizers.Adadelta(),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "model_cnn.summary()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong><font color=darkorange> Solution 5 </font></strong>: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convolutional layer with 32 filters and 3 * 3 kernel sizes and 'relu' activation (use the `Conv2D` object)\n",
    "# Convolutional layer with 64 filters and 3 * 3 kernel sizes and 'relu' activation (use the `Conv2D` object)\n",
    "# Max pooling with pool size 2 * 2 (use the `MaxPooling2D` object)\n",
    "# Dropout with probability 0.25 (use the `Dropout` object)\n",
    "# Dense layer with 128 units with relu activation\n",
    "# Dropout with probability 0.5\n",
    "# Dense output layer with softmax activation\n",
    "\n",
    "model_cnn = Sequential()\n",
    "\n",
    "model_cnn.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "model_cnn.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "model_cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_cnn.add(Dropout(0.25))\n",
    "model_cnn.add(Flatten())\n",
    "model_cnn.add(Dense(128, activation='relu'))\n",
    "model_cnn.add(Dropout(0.5))\n",
    "model_cnn.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model_cnn.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=keras.optimizers.Adadelta(),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "model_cnn.summary()\n",
    "send('model_cnn', 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong><font color=darkorange> Q6 </font></strong>:fit the NN over 5 epochs and plot the evolution of  the error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong><font color=darkorange> Solution 6 </font></strong>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 5\n",
    "\n",
    "# Run the train\n",
    "history = model_cnn.fit(x_train, y_train,\n",
    "                        batch_size=batch_size,\n",
    "                        epochs=epochs,\n",
    "                        verbose=1,\n",
    "                        validation_data=(x_test, y_test))\n",
    "score = model_cnn.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 5))\n",
    "plt.plot(history.epoch, history.history['accuracy'], lw=3, label='Training')\n",
    "plt.plot(history.epoch, history.history['val_accuracy'], lw=3, label='Testing')\n",
    "plt.legend(fontsize=14)\n",
    "plt.title('Accuracy of CNN regression', fontsize=16)\n",
    "plt.xlabel('Epoch', fontsize=14)\n",
    "plt.ylabel('Accuracy', fontsize=14)\n",
    "plt.tight_layout()\n",
    "send(plt, 9)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
